# Data Science

The following are notes and communications around our investigations and use for data science, machine learning and computer vision behind the scenes of the [Barnes Collection Website](https://collection.barnesfoundation.org).

# Sam Hains

I began my research by exploring the possibility of training an object
detection model based on the list of classes that Shelley had given me.
I created a training set of around 300,000 images using the Microsoft
Bing api for each of the classes that was set up for my by Steven.

In order to be able to train an object detection model you need
'bounding box' information for every image class, which locates where
the thing is within the image.  Typically, this task is done manually,
by humans drawing a box around the item within the image. With 300,000
images to draw bounding box information for, this wasn't feasible so I
wanted to see if there was a way to use deep-learning techniques to
automate this.

I found research done by MIT for using deep networks to do localisation
http://cnnlocalization.csail.mit.edu/. It seemed possible that bounding
boxes could created based on heatmap data generated by neural networks.
I forked a keras version of this project, and made some adjustments and
fixes to get it running (https://github.com/samhains/keras-cam).   I ran
some tests on a basic 'cats' vs 'dogs' model, to find that it worked OK
but not very well,  so it felt a little too risky and time intensive of
an option to pursue at this point.  (Screenshot attached).

There is some work being done around a more collaborative approach where
humans and machine learning algorithms collaborate to solve this problem
we were facing (https://arxiv.org/abs/1602.08405)! Basically, this
involves humans participating in the training of the AI in real time, as
they 'click' to give hints to the machine learning to place 'bounding
boxes'.   I worked out this was still likely to be a lot of manual
labour for whoever was doing the clicking (10 hours labour at least)
given size of our dataset.  Furthermore, the need for another interface
to be built, and the experimental nature of this research made it
another risky option.

I started to explore options that did not require bounding-box
information. I began by modifying an image classification model to
produce multiple results. I was immediately impressed by the results,
and considered that with some careful training this could be the best
option within our time-frame and budget.

I started test developing our first custom classification models
locally, on around 20 different classes of thing. I experimented with
using some 'oil paint filter' pre-processing techniques on the training
images so that they looked more like the images in the Barnes
collection, but I found this produced mixed results. Sometimes it would
classify objects more clearly, but when it was wrong - it was very
wrong! I decided not to use any 'oil paint' pre-processing on the
training data.

During this test phase, I also did some tests to determine whether
transfer learning or fine tuning techniques would be helpful for our
dataset.  The results were that fine-tuning, with pre-trained VGG16
weights seemed to give the best results which makes sense given the size
of our data-set (quite large).

I moved into the production environment and trained my first custom
model on the server provided to me by Barnes Foundation staff. The results were quite good, although I found that there were a few things that needed to be re-considered:

Some classes, such as 'cupids' and 'harlequin' seemed to be getting
triggered constantly which was warping the results.  These classes
needed to be pruned.

Humorously, it seemed that the 'christ' class seemed to be picking up
any bearded man.   I had to go back and look at the dataset again.
Certain classes, such as 'greek' had many images in them that were
simply not relevant to detection, so I either removed them, or used a
different Bing search to capture what we were after.

I also added data to some classes that weren't getting picked up. For
example, 'Man' and 'Woman' didn't seem to get triggered at all in this
first model. After looking again at the training data it made sense why
this was happening - the machine had learned what a 'man' was from a set
of very glossy and modern Bing search images. I added images from '1800s
man', '1700s man', '1600s man' etc. to the man category to give it more
of a broad understanding.

I also added new classes that could better pick up on some of the more
pre-modern tropes such as 'reclining nude' and 'women in fields'.

I trained a second custom model and allowed it to run for around 2 days
(the first custom model was trained for a day).

The results are an improvement over the 'out-of-the-box' results we saw
in Project 1. Occasionally, I am impressed that the machine has spotted
something, or suggested a theme that I may not have seen myself.
However, it is still far from perfect, many images are still not
interpreted correctly -  there is room for improvement.

There are ways that we could improve these results:
- A third model using the existing approach could be trained based on
feedback from the results of the second.
- Development of the human/ai collaborative object detection training
interface discussed earlier - however this would involve  a lengthy
development cost.
